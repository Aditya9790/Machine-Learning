{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Boosting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnfcv/ylScLH350SvV1E3A"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting** \n",
        "\n",
        "The **principle** behind boosting algorithms is that **first we build a model on the training dataset, then a second model is built to rectify the errors present in the first model.** \n",
        "\n",
        "This procedure is continued until and unless the **errors are minimized**, and the dataset is predicted correctly. \n",
        "\n",
        "In particular, we start with a weak model and subsequently, each new model is fit on a modified version of the original dataset.\n",
        "\n",
        "**Notes :**\n",
        "* A weak learner is a model that performs at least slightly better than a random model.\n",
        "\n",
        "* Decision trees are generally used as weak learners in gradient boost.\n",
        "\n",
        "* Unlike AdaBoost where decision trees with only one level (decision stumps) are used, decision trees in Gradient Boost generally consists of some 3-7 levels."
      ],
      "metadata": {
        "id": "DJYLhe5EdvK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps :**\n",
        "\n",
        "Following are the steps involved :\n",
        "\n",
        "1. Make a first guess for y_train and y_test, using the average of y_train.\n",
        "\n",
        "\\begin{align}\n",
        "y_{train_{p0}} = \\frac{1}{n}\\sum_{i=1}^n y_{train_t}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "y_{test_{p0}} = y_{train_{p0}}\n",
        "\\end{align}\n",
        "\n",
        "2. Calculate the residuals from the training dataset. \n",
        "\n",
        "\\begin{align}\n",
        "r_0 = y_{train} - y_{train_{p0}}\n",
        "\\end{align}\n",
        "\n",
        "3. Fit a weak learner to the residuals minimizing the loss function. Let's call it *f0*.\n",
        "\n",
        "\\begin{align}\n",
        "r_0 = f_0(X_{train})\n",
        "\\end{align}\n",
        "\n",
        "4. Increment the predicted y's. \n",
        "\n",
        "\\begin{align}\n",
        "y_{train_{p1}} = y_{train_{p0}} + \\alpha f_0(X_{train})\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "y_{test_{p1}} = y_{test_{p0}} + \\alpha f_0(X_{test})\n",
        "\\end{align}\n",
        "\n",
        "where, Î± is the learning rate.\n",
        "\n",
        "5. Repeat steps 2 through 4 until you reach the number of boosting rounds."
      ],
      "metadata": {
        "id": "bNK6AbY9eCOL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQxXPv1HdrIk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GradBoost(model, X_train, y_train, X_test, y_test, \n",
        "              boosting_rounds, learning_rate:float = 0.1):\n",
        "    \n",
        "    # make initial guess on y_test_hat and y_train_hat \n",
        "    # the guess is mean of X_train\n",
        "    y_hat_train = np.repeat(np.mean(y_train), len(y_train))\n",
        "\n",
        "    y_hat_test = np.repeat(np.mean(y_train), len(y_test))\n",
        "\n",
        "    # calculate residual from training data\n",
        "    # for initial guess\n",
        "\n",
        "    residuals = y_train - y_hat_train\n",
        "\n",
        "    # iterate over boosting rounds\n",
        "    # fit model for residuals as target variable\n",
        "    for i in range(boosting_rounds):\n",
        "        model = model.fit(X_train, residuals)\n",
        "\n",
        "        # increment predicted training y with interval\n",
        "        # learning_rate * model\n",
        "        y_hat_train += (learning_rate * model.predict(X_train))\n",
        "\n",
        "        # preform same for test set\n",
        "        y_hat_test += (learning_rate * model.predict(X_test))\n",
        "\n",
        "        # calculate residual for next round\n",
        "        residuals = y_train - y_hat_train\n",
        "    \n",
        "    return y_hat_train, y_hat_test"
      ],
      "metadata": {
        "id": "Ir2LVRPteLZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-8tDGjt4jCOv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}