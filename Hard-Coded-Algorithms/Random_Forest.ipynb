{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forest.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMG7vECBIZHDjYh6oluUPTX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forests** \n",
        "\n",
        "Random Forest is a **bagging** technique that **trains multiple decision trees** with **minor modification** in split criterion.\n",
        "\n",
        "In case of decision trees, we train a single decision tree. \n",
        "\n",
        "In random forest we train multiple decision trees on different training sets obtained through bagging (aka Boostrap Agreggation)."
      ],
      "metadata": {
        "id": "dZ3l3EC2qUBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm**\n",
        "**Input**:\n",
        "\n",
        "1. The training data $D$ with shape $(n,m)$, say $D_1,D_2,\\ldots ,D_q$ with replacement from $D$.\n",
        "\n",
        "2. In each of the datasets $D_j$, select $u$ out of $m$ where $u \\le m$ features before each split and train a full decision tree $h_j(\\mathbf x)$\n",
        "\n",
        "3. The final predictor : \n",
        "\n",
        "  * For regression, an average output from $q$ regressors is assigned to the new example:\n",
        "  \n",
        "  $$ h(\\mathbf x) = \\frac {1}{q} \\sum_{j=1}^q h_j(\\mathbf x) $$\n",
        "\n",
        "  * For classification, a majority voting is taken and the class label with the maximum number of votes is assigned to the new example."
      ],
      "metadata": {
        "id": "ZvcO-rc_sP8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation**\n",
        "\n",
        "In order to keep the implementation focussed to main components of random forest, we make use of `DecisionTreeClassifier` from scikit-learn library for the decision tree components."
      ],
      "metadata": {
        "id": "HIsYO7SMsv8S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "scJUMf6nnzBY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.RandomState(1)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Bagging**\n",
        "\n",
        "We define a function for bagging - creating boostrap samples $D_1,D_2,\\ldots,D_q$ from the original dataset $D:$\n",
        "\n",
        "The **key step** is : \n",
        "\n",
        "`np.random.choice` with **size=n_samples** and **replace=True** \n",
        "\n",
        "It ensures that the boostrapped sample has the same number of samples as the original dataset and it is obtained by sampling with replacement."
      ],
      "metadata": {
        "id": "ccyldqLfszGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag(X, y):\n",
        "    # Count rows\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    # generate samples from input\n",
        "    indices = np.random.choice(n_samples, size=n_samples,\n",
        "                               replace=True)\n",
        "    return X[indices], y[indices]"
      ],
      "metadata": {
        "id": "r8MpJYgZsyXM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_samples = 5\n",
        "#     # generate samples from input\n",
        "# indices = np.random.choice(n_samples, size = n_samples,\n",
        "#                             replace=True)\n",
        "# indices"
      ],
      "metadata": {
        "id": "McLm-X8mtdbN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Majority Voting**\n",
        "\n",
        "Count most common label to obtain majority vote for class labels."
      ],
      "metadata": {
        "id": "dBGYXLJ9wJKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = [1, 1, 1, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3]\n",
        "print(Counter(y))\n",
        "print(Counter(y).most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbIZAfm1woby",
        "outputId": "098565e9-a506-48e9-8a1c-8db365222cb9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({3: 5, 2: 4, 1: 3, 0: 2})\n",
            "[(3, 5), (2, 4), (1, 3), (0, 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# most_common returns n most frequent elem count\n",
        "Counter(y).most_common(1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mKNkf2fwvn4",
        "outputId": "7aa67940-25cc-4048-e90d-59f21a88ff6f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(y).most_common()[1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TrxjyzvxT5H",
        "outputId": "1df35dcc-5823-4dd4-abea-1701be0c1de0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def most_comon_label(y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0][0]\n",
        "    return most_common"
      ],
      "metadata": {
        "id": "3jsRed2wtstK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random forest class**\n",
        "\n",
        "We create a `RandomForest` class with the following default parameters :\n",
        "\n",
        "* number of trees = 10\n",
        "\n",
        "* minimum number of samples = 2\n",
        "\n",
        "* maximum depth = 100 \n",
        "\n",
        "The `max_features` is a configurable parameter that can be set by the user."
      ],
      "metadata": {
        "id": "VhzVm9omyRjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "    def __init__(self, n_trees = 10, min_sample_split = 2,\n",
        "                 max_depth = 100, max_features = None):\n",
        "        \n",
        "        # hpt for fixing number of trees\n",
        "        self.n_trees = n_trees\n",
        "\n",
        "        # max depth\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "        # min sample split\n",
        "        self.min_sample_split = min_sample_split\n",
        "\n",
        "        # max features\n",
        "        self.max_features = max_features\n",
        "        self.trees = []\n"
      ],
      "metadata": {
        "id": "SSGt8oizxa8g"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Random Forest**\n",
        "We implement the `fit` method.\n",
        "\n",
        "* Initialize an empty list of decision tree classifiers.\n",
        "\n",
        "* In the `for` loop, we train each decision tree with parameters set from random forest on a boostrapped sample obtained through the `bag` function."
      ],
      "metadata": {
        "id": "T-vOlqGo_oqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self, X, y):\n",
        "    # empty array to store trees\n",
        "    self.trees = []\n",
        "\n",
        "    for _ in range(self.n_trees):\n",
        "        tree = DecisionTreeClassifier(\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            max_depth=self.max_depth,\n",
        "            max_features = self.max_features)\n",
        "        \n",
        "        X_sample, y_sample = bag(X, y)\n",
        "        tree.fit(X_sample, y_sample)\n",
        "        # append each of trees in list\n",
        "        self.trees.append(tree)"
      ],
      "metadata": {
        "id": "cO53inI8_oVg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**\n",
        "Let's implement `predict` function : \n",
        "\n",
        "Here, we need to note that each of the trees will give predictions for all the individual rows of the input data.\n",
        "\n",
        "**For example :** If we have a random forest with 3 trees and 2 classes 0 & 1, let's assume the prediciton for 5 samples is as follows : \n",
        "\n",
        "* Tree 1 gives 00111\n",
        "\n",
        "* Tree 2 gives 11001\n",
        "\n",
        "* Tree 3 gives 10101\n",
        "\n",
        "We need to aggregate the output for the respective samples and take an average / majority vote. For this, we will use `np.swapaxes`."
      ],
      "metadata": {
        "id": "hEipg2-qZOcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, X):\n",
        "    tree_predict = np.array([tree.predict(X) for tree in self.trees])\n",
        "\n",
        "    # each tree will give predictions\n",
        "    tree_predict = np.swapaxes(tree_predict, 0, 1)\n",
        "\n",
        "    # get most common lsbel for each class\n",
        "    y_pred = [most_comon_label(tree_pred) for tree_pred in tree_predict]\n",
        "    return np.array(y_pred)"
      ],
      "metadata": {
        "id": "XlvNai_6_eAs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine all components**"
      ],
      "metadata": {
        "id": "T8I4FAqPaUSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag(X, y):\n",
        "    n_samples = X.shape[0]\n",
        "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "    return X[indices], y[indices]\n",
        "\n",
        "\n",
        "def most_common_label(y):\n",
        "  counter = Counter(y)\n",
        "  most_common = counter.most_common(1)[0][0]\n",
        "  return most_common\n",
        "\n",
        "\n",
        "class RandomForest:\n",
        "  def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, max_features=None):\n",
        "    self.n_trees = n_trees  \n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.max_depth = max_depth  \n",
        "    self.max_features = max_features \n",
        "    self.trees = []\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.trees = []\n",
        "\n",
        "    for _ in range(self.n_trees):\n",
        "      tree = DecisionTreeClassifier(  \n",
        "          min_samples_split=self.min_samples_split,\n",
        "          max_depth=self.max_depth,\n",
        "          max_features=self.max_features)\n",
        "\n",
        "      X_sample, y_sample = bag(X, y)\n",
        "      tree.fit(X_sample, y_sample)\n",
        "      self.trees.append(tree)  \n",
        "\n",
        "  def predict(self, X):\n",
        "    tree_predict = np.array([tree.predict(X) for tree in self.trees])\n",
        "    tree_predict = np.swapaxes(tree_predict, 0, 1)\n",
        "\n",
        "    y_pred = [most_common_label(tree_pred) for tree_pred in tree_predict]\n",
        "    return np.array(y_pred)"
      ],
      "metadata": {
        "id": "FJKBRP4OaR4q"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tCdCsKAMab6v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
